<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> | Jordie Shier</title> <meta name="author" content="Jordie Shier"/> <meta name="description" content="Jordie Shier's personal portfolio website "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icons8-electronic-music-96.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jordieshier.com/adc2024_dev/"> <link href="https://vjs.zencdn.net/8.16.1/video-js.css" rel="stylesheet"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Yantramanav:wght@100;300;400;500;700;900&display=swap" rel="stylesheet"> </head> <body class="external"> <div class="external yantramanav-regular"> <div class="container mt-5"> <h1 id="torchdrum">TorchDrum</h1> <p>An audio-driven 808 drum synthesiser built with PyTorch and JUCE</p> <div> Jordie Shier<sup>1</sup>, Charalampos Saitis<sup>1</sup>, Andrew Robertson<sup>2</sup>, and Andrew McPherson<sup>3</sup> <div style="font-size:small"> <sup>1</sup>Centre for Digital Music, Queen Mary University of London<br> <sup>2</sup>Ableton AG<br> <sup>3</sup>Dyson School of Design Engineering, Imperial College London<br> </div> </div> <hr> <div align="left"> <a href="https://github.com/jorshi/ddsp-timbre-remap" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i> <b>Training Code</b></a> | <a href="https://drive.google.com/file/d/1m-fi_L9U__gK7VpXv2tWOnBkRhUej08r/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><i class="fas fa-headphones"></i> <b>Audio Plugin</b></a> | <a href="https://colab.research.google.com/drive/1nwV5y2eYiCF9YIM1BSmKU9uiPBbw9OxV?usp=sharing" target="_blank" rel="noopener noreferrer"><i class="fas fa-laptop-code"></i> <b>Google Colab</b></a> | <a href="https://arxiv.org/abs/2407.04547" target="_blank" rel="noopener noreferrer"><i class="fas fa-paperclip"></i> <b>Paper</b></a> </div> <hr> <h2 id="what-is-torchdrum">What is TorchDrum?</h2> <p>TorchDrum is an open-source audio plugin that transforms input signals into a synthesised 808 drum in real-time. The rhythm, loudness, and timbral variations from a percussive input signal are mapped to synthesiser controls, allowing for dynamic, audio-based control. The figure above shows the main signal flow implemented in the audio plug-in, which was built in C++ using JUCE.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/adc2024/real-time-slide-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/adc2024/real-time-slide-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/adc2024/real-time-slide-1400.webp"></source> <img src="/assets/img/adc2024/real-time-slide.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Real-time Plugin Oveview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> The main signal processing and mapping components are:</p> <ul> <li>1) <strong>Onset Detection</strong>: drum hits in the input signal are detected using an onset detection algorithm. This allows the rhythm from the input to be mapped to the synthesiser.</li> <li>2) <strong>Feature Extraction</strong>: a short segment of audio is extracted from the input signal for feature extraction. We only use 256 samples which is ~5.3ms (48kHz sampling rate), this minimises the delay between a detected onset and triggering the synth. This segment of 256 samples is passed into an audio feature extractor that measures the loudness, spectral centroid (brightness), and spectral flatness (noisiness) of the input.</li> <li>3) <strong>Parameter Mapping Neural Network</strong>: These audio features are then passed into a neural network that has been trained to produce synthesiser parameter modulations for the 808 synthesiser so the output follows the variations in loudness and timbre observed at the input.</li> </ul> <p>We refer to the process of automatically updating synthesiser parameters to reflect the timbral variations in the input signal as <em>timbre remapping</em>. <br></p> <hr> <h2 id="what-is-timbre-remapping">What is Timbre Remapping?</h2> <p>Timbre remapping involves the analysis and transfer of timbre from an audio input onto controls for a synthesizer. To achieve timbre remapping we consider musical phrases where timbre changes from note to note. We look at the <em>differences in timbre</em> (and loudness) between notes and try to recreate those differences on our synthesiser. The process is similar to transposing a melody into a different key, but instead of pitch, we are dealing with melodies of timbre and loudness, and are transposing from an input instrument onto synthesiser.</p> <p>Let’s make this a bit more concrete. Here is an example of a short snare drum phrase with plots of the timbre and loudess for each hit. The arrow shows differences between each note compared to the first note in the phrase.</p> <iframe width="770" height="440" src="https://www.youtube.com/embed/X8ivQVtUQQc?si=bO6a6xFqT-2sFDlX&amp;loop=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>Here’s the same snare drum phase (without the rhythm) followed by all the differences in timbre and loudness mapped onto a synthesiser. What we end up is a new a synthesiser sound (a modulated preset) for each hit in the input phrase.</p> <iframe width="770" height="440" src="https://www.youtube.com/embed/cdY3EB3O7TM?si=GGN6hkTJd0HjTJhh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p><br></p> <hr> <h2 id="how-does-the-parameter-mapping-neural-network-work">How does the Parameter Mapping Neural Network Work?</h2> <p>The neural network enables real-time timbre remapping, allowing for input signals to be transformed with low-latency. The neural network we use is relatively small, but it contains non-linearities, allowing for complex relationships between input features and parameters to be formed.</p> <p>The neural network is trained to generate synthesiser parameters to match the loudness and timbral changes over full the full length of a drum hit (~1 second). During real-time operation we can’t wait a full second to see how the input sound fully evolves, so we predict parameters based on short segments of audio at detected onsets.</p> <p>We also employ a recent method called <em>differentiable digital signal processing</em> (DDSP), which enables DSP algorithms to be integrated directly into gradient-descent based optimization used to train neural networks. Practically speaking, this allows us to compute training error on the audio output of our synthesiser using loudness and timbral features. We don’t need to know the parameters for our timbre remapping ahead of time – we only need our input audio which we use during <em>self-supervised</em> training.</p> <p>This figure outlines the training process:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/adc2024/neural-net-training-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/adc2024/neural-net-training-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/adc2024/neural-net-training-1400.webp"></source> <img src="/assets/img/adc2024/neural-net-training.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Neural Network Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> The main takeaway from the figure is that we’re training the neural network to match <em>differences</em> in audio features between two difference hits. \(y = f(x_a) - f(x_b)\) is the difference between two different sounds in an input, and \(\hat{y} = f(x_c) - f(x_d)\) is the difference between a synthesiser preset and a modulated version of that preset. The goal of the neural network is create a parameter modulation \(\theta_{mod}\) so that \(y = \hat{y}\). Furthermore, the neural network is trained to make this prediction from onset features \(f_o(\cdot)\) to allow for real-time parameter prediction.</p> <p><br></p> <hr> <h2 id="creating-a-plugin">Creating a Plugin</h2> <p>Neural network training is conducted using PyTorch</p> <p><br></p> <hr> <h2 id="plugin-overview">Plugin Overview</h2> <div class="image-container"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/adc2024/plugin-ui-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/adc2024/plugin-ui-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/adc2024/plugin-ui-1400.webp"></source> <img src="/assets/img/adc2024/plugin-ui.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="MLP Parameter Mapping Overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="plus-button" data-info="Audio features are extracted from input audio at detected onsets. These features (visualized in the radar plot) are passed as input to a neural network, which generates modulations for synthesis parmeters." style="top: 12%; left: 37%;">+</div> <div class="plus-button" data-info="The drum synthesizer is modeled on an 808 snare drum. These parameters modify the sound and are modulated by a neural network. The inner ring controls the parameter value outer ring displays the modulation applied by the neural net." style="top: 15%; left: 67%;">+</div> <div class="plus-button" data-info="Onset detection is used to detect percussive events in input audio. The graph shows the amplitude envelope of the input and controls below are used to adjust detection parameters." style="top: 65%; left: 45%;">+</div> <div class="plus-button" data-info="Global controls. Dry/Wet allows for mixing of input audio and the drum synth. 'Neurality' is the strength of the neural network modulation -- zero is no modulation and increasing values lead to more extreme reaction to the input" style="top: 67%; left: 7%;">+</div> <div class="plus-button" data-info="Load a preset and modulation neural network. Neural network models are trained offline to react to input audio and modulate parameters of a preset to match the dynamic and timbral expression of a performance. Train your own models on Google Colab or experiment with pre-trained models." style="top: 5%; left: 2%;">+</div> </div> <div id="info-box" style="display: none;"> <p id="info-content"></p> <button onclick="closeInfo()">Close</button> </div> <p><br></p> <h2 id="what-is-torchdrum-1">What is TorchDrum?</h2> <iframe width="770" height="440" src="https://www.youtube.com/embed/IVaNfp4rev0?si=_Ja3pAfqm_34z5bH" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="/assets/js/image_interact.js"></script> </body> </html>