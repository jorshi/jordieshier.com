---
Articles
---

@article{caspe2025designing,
  abbr={JAES},
  title={Designing Neural Synthesizers for Low Latency Interaction},
  author={Caspe, Franco and Shier, Jordie and Sandler, Mark and Saitis, Charalampos and McPherson, Andrew},
  journal={Journal of the Audio Engineering Society},
  year={2025},
  arxiv={2503.11562},
  abstract={Neural Audio Synthesis (NAS) models offer interactive musical control over high-quality, expressive audio generators. While these models can operate in real-time, they often suffer from high latency, making them unsuitable for intimate musical interaction. The impact of architectural choices in deep learning models on audio latency remains largely unexplored in the NAS literature. In this work, we investigate the sources of latency and jitter typically found in interactive NAS models. We then apply this analysis to the task of timbre transfer using RAVE, a convolutional variational autoencoder for audio waveforms introduced by Caillon et al. in 2021. Finally, we present an iterative design approach for optimizing latency. This culminates with a model we call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is low-latency and exhibits better pitch and loudness replication while showing timbre modification capabilities similar to RAVE. We implement it in a specialized inference framework for low-latency, real-time inference and present a proof-of-concept audio plugin compatible with audio signals from musical instruments. We expect the challenges and guidelines described in this document to support NAS researchers in designing models for low-latency inference from the ground up, enriching the landscape of possibilities for musicians.},
  code={https://github.com/fcaspe/BRAVE},
  link1_url={https://fcaspe.github.io/BravePlugin/},
  link1_text={Plugin},
}

@inproceedings{saitis2024ethnographic,
  abbr={CHIME},
  title={Ethnographic Exploration of Timbre in Hackathon Designs},
  author={Saitis, Charalampos and Del Sette, Bleiz Macsen and Tian, Haokun and Shier, Jordie and Zheng, Shuoyang and Skach, Sophie and Reed, Courtney N and Ford, Corey},
  year={2024},
  booktitle={CHIME Annual Conference},
  pdf={Saitis_CHIME24_TimbreEthno.pdf},
  abstract={This paper reports a summary account of the Timbre Tools Hackathon: a hackathon that invited audio developers and music technologists to consider and work with timbre through the design of tools that promote a timbre-first approach to digital instrument craft practice—timbre tools. Through ethnographic observation, we identified different approaches towards integrating timbre as an active part of creating tools and technologies in music. These strategies inform future work and the development of tools to assist awareness and exploration of timbre for instrument makers.}
}

@inproceedings{saitis2024timbre,
  abbr={AM},
  author = {Saitis, Charalampos and Del Sette, Bleiz M and Shier, Jordie and Tian, Haokun and Zheng, Shuoyang and Skach, Sophie and Reed, Courtney N. and Ford, Corey},
  title = {Timbre Tools: Ethnographic Perspectives on Timbre and Sonic Cultures in Hackathon Designs},
  year = {2024},
  abstract = {Timbre is a nuanced yet abstractly defined concept. Its inherently subjective qualities make it challenging to design and work with. In this paper, we propose to explore the conceptualisation and negotiation of timbre within the creative practice of timbre tool makers. To this end, we hosted a hackathon event and performed an ethnographic study to explore how participants engaged with the notion of timbre and how their conception of timbre was shaped through social interactions and technological encounters. We present individual descriptions of each team’s design process and reflect on our data to identify commonalities in the ways that timbre is understood and informed by sound technologies and their surrounding sonic cultures, e.g., by relating concepts of timbre to metaphors. We further current understanding by offering novel interdisciplinary and multimodal insights into understandings of timbre.},
  booktitle = {Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3678299.3678322}
}

@inproceedings{shier2024real,
  abbr={NIME},
  title={Real-time timbre remapping with differentiable DSP},
  author={Shier, Jordie and Saitis, Charalampos and Robertson, Andrew and McPherson, Andrew},
  year={2024},
  booktitle={New Interfaces for Musical Expression},
  arxiv={2407.04547},
  abstract={Timbre is a primary mode of expression in diverse musical contexts. However, prevalent audio-driven synthesis methods predominantly rely on pitch and loudness envelopes, effectively flattening timbral expression from the input. Our approach draws on the concept of timbre analogies and investigates how timbral expression from an input signal can be mapped onto controls for a synthesizer. Leveraging differentiable digital signal processing, our method facilitates direct optimization of synthesizer parameters through a novel feature difference loss. This loss function, designed to learn relative timbral differences between musical events, prioritizes the subtleties of graded timbre modulations within phrases, allowing for meaningful translations in a timbre space. Using snare drum performances as a case study, where timbral expression is central, we demonstrate real-time timbre remapping from acoustic snare drums to a differentiable synthesizer modeled after the Roland TR-808.},
  code={https://github.com/jorshi/ddsp-timbre-remap},
  website={https://jordieshier.com/projects/nime2024/},
  video={https://youtu.be/IVaNfp4rev0?si=ifgdlZUGaeSif7ug},
  link1_url={https://drive.google.com/file/d/1m-fi_L9U__gK7VpXv2tWOnBkRhUej08r/view?usp=drive_link},
  link1_text={Plugin}
}

@article{hayes2024review,
  abbr={Frontiers},
  author={Hayes, Ben and Shier, Jordie and Fazekas, György and McPherson, Andrew and Saitis, Charalampos},   
  title={A review of differentiable digital signal processing for music and speech synthesis},      
	journal={Frontiers in Signal Processing},      
	volume={3},           
	year={2024},
	pdf={https://www.frontiersin.org/articles/10.3389/frsip.2023.1284100},       
  abstract={The term “differentiable digital signal processing” describes a family of techniques in which loss function gradients are backpropagated through digital signal processors, facilitating their integration into neural networks. This article surveys the literature on differentiable audio signal processing, focusing on its use in music and speech synthesis. We catalogue applications to tasks including music performance rendering, sound matching, and voice transformation, discussing the motivations for and implications of the use of this methodology. This is accompanied by an overview of digital signal processing operations that have been implemented differentiably, which is further supported by a web book containing practical advice on differentiable synthesiser programming (<ext-link ext-link-type="uri" xlink:href="https://intro2ddsp.github.io/" xmlns:xlink="http://www.w3.org/1999/xlink">https://intro2ddsp.github.io/</ext-link>). Finally, we highlight open challenges, including optimisation pathologies, robustness to real-world conditions, and design trade-offs, and discuss directions for future research.}
}

@inproceedings{shier_2023_differentiable,
  abbr={EAA},
	title = {Differentiable Modelling of Percussive Audio with Transient and Spectral Synthesis},
	author = {Shier, Jordie and Caspe, Franco and Robertson, Andrew and Sandler, Mark and Saitis, Charalampos and McPherson, Andrew},
	equal = {Shier, Jordie and Caspe, Franco},
  year = 2023,
	booktitle = {Proceedings of the 10th Convention of the European Acoustics Association Forum Acusticum 2023},
  pdf={shier2023differentiable_paper.pdf},
  code={https://github.com/jorshi/drumblender},
  website={https://jordieshier.com/projects/differentiable_transient_synthesis/},
  abstract = {Differentiable digital signal processing (DDSP) techniques, including methods for audio synthesis, have gained attention in recent years and lend themselves to interpretability in the parameter space. However, current differentiable synthesis methods have not explicitly sought to model the transient portion of signals, which is important for percussive sounds. In this work, we present a unified synthesis framework aiming to address transient generation and percussive synthesis within a DDSP framework. To this end, we propose a model for percussive synthesis that builds on sinusoidal modeling synthesis and incorporates a modulated temporal convolutional network for transient generation. We use a modified sinusoidal peak picking algorithm to generate time-varying non-harmonic sinusoids and pair it with differentiable noise and transient encoders that are jointly trained to reconstruct drumset sounds. We compute a set of reconstruction metrics using a large dataset of acoustic and electronic percussion samples that show that our method leads to improved onset signal reconstruction for membranophone percussion instruments.}
}

@inproceedings{shier2022dmrn,
  abbr={DMRN},
  title={Real-time timbre mapping for synthesized percussive performance},
  author={Shier, Jordie},
  booktitle={DMRN+17: Digital Music Research Network One-day Workshop 2022},
  year={2022},
  poster={shier2022dmrn_poster.pdf}
}

@article{turian2022hear,
  abbr={arXiv},
  abstract={What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without fine-tuning? The aim of the HEAR benchmark is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR evaluates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. HEAR was launched as a NeurIPS 2021 shared challenge. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.},
  title={Hear 2021: Holistic evaluation of audio representations},
  author={Turian, Joseph and Shier, Jordie and Khan, Humair Raj and Raj, Bhiksha and Schuller, Bj{\"o}rn W and Steinmetz, Christian J and Malloy, Colin and Tzanetakis, George and Velarde, Gissel and McNally, Kirk and Henry, Max and Pinto, Nicolas and Noufi, Camille and Clough, Christian and Herremans, Dorien and Fonseca, Eduardo and Engel, Jesse and Salamon, Justin and Esling, Philippe and Manocha, Pranay and Watanabe, Shinji and Jin, Zeyu and Bisk, Yonatan},
  journal={arXiv preprint},
  year={2022},
  arxiv={2203.03022}
}

@inproceedings{turian2021one,
  abbr={DAFx},
  abstract={We release synth1B1, a multi-modal audio corpus consisting of 1 billion 4-second synthesized sounds, paired with the synthesis parameters used to generate them. The dataset is 100x larger than any audio dataset in the literature. We also introduce torchsynth, an open source modular synthesizer that generates the synth1B1 samples on-the-fly at 16200x faster than real-time (714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth timbre and subtractive synth pitch. Using these datasets, we demonstrate new rank-based evaluation criteria for existing audio representations. Finally, we propose a novel approach to synthesizer hyperparameter optimization.},
  title={One billion audio sounds from gpu-enabled modular synthesis},
  author={Turian, Joseph and Shier, Jordie and Tzanetakis, George and McNally, Kirk and Henry, Max},
  equal={Turian, Joseph and Shier, Jordie},
  booktitle={2021 24th International Conference on Digital Audio Effects (DAFx)},
  pages={222--229},
  year={2021},
  organization={IEEE},
  pdf={https://dafx2020.mdw.ac.at/proceedings/papers/DAFx20in21_paper_34.pdf},
  video={https://dafx2020.mdw.ac.at/proceedings/presentations/DAFx20in21_paper_34.mp4},
  code={https://github.com/torchsynth/torchsynth},
  website={https://torchsynth.readthedocs.io/en/latest/}
}

@article{shier2021manifold,
  abbr={JAES},
  abstract={The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The task of organizing and selecting from these large collections can be challenging and time consuming, which points to the need for improved methods for user interaction. This paper presents a system that computationally characterizes and organizes drum machine samples in two dimensions based on sound similarity. The goal of the work is to support the development of intuitive drum sample browsing systems. The methodology presented explores time segmentation, which isolates temporal subsets from the input signal prior to audio feature extraction, as a technique for improving similarity calculations. Manifold learning techniques are compared and evaluated for dimensionality reduction tasks, and used to organize and visualize audio collections in two dimensions. This methodology is evaluated using a combination of objective and subjective methods including audio classification tasks and a user listening study. Finally, we present an open-source audio plug-in developed using the JUCE software framework that incorporates the findings from this study into an application that can be used in the context of a music production environment.},
  title={Manifold learning methods for visualization and browsing of drum machine samples},
  author={Shier, Jordie and McNally, Kirk and Tzanetakis, George and Brooks, Ky Grace},
  journal={Journal of the Audio Engineering Society},
  volume={69},
  number={1/2},
  pages={40--53},
  year={2021},
  pdf={shier2021manifold_greenoa.pdf},
  publisher={Audio Engineering Society},
  link1_url={https://github.com/jorshi/sample_analysis},
  link1_text={Analysis Code},
  link2_url={https://github.com/jorshi/sieve},
  link2_text={Audio Plugin}
}

@inproceedings{shier2020spiegelib,
  abbr={AES},
  abstract={Automatic synthesizer programming is the field of research focused on using algorithmic techniques to generate parameter settings and patch connections for a sound synthesizer. In this paper, we present the Synthesizer Programming with Intelligent Exploration, Generation, and Evaluation Library (spiegelib), an open-source, object oriented software library to support continued development, collaboration, and reproducibility within this field. spiegelib is designed to be extensible, providing an API with classes for conducting automatic synthesizer programming research. The name spiegelib was chosen to pay homage to Laurie Spiegel, an early pioneer in electronic music. In this paper we review the algorithms currently implemented in spiegelib, and provide an example case to illustrate an application of spiegelib in automatic synthesizer programming research.},
  title={Spiegelib: An automatic synthesizer programming library},
  author={Shier, Jordie and Tzanetakis, George and McNally, Kirk},
  booktitle={Audio Engineering Society Convention 148},
  year={2020},
  organization={Audio Engineering Society},
  pdf={https://www.aes.org/e-lib/online/browse.cfm?elib=20794},
  poster={shier2020spiegelib_poster.pdf},
  code={https://github.com/spiegelib/spiegelib},
  website={https://spiegelib.github.io/spiegelib/}
}

@inproceedings{shier2017analysis,
  abbr={AES},
  abstract={The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. The development of new tools to assist users organizing and managing libraries of this type requires comprehensive audio analysis that is distinct from that used for general classification or onset detection tasks. In this paper 4230 kick and snare samples, representing 250 individual electronic drum machines are evaluated. Samples are segmented into different lengths and analyzed using comprehensive audio feature analysis. Audio classification is used to evaluate and compare the effect of this time segmentation and establish the overall effectiveness of the selected feature set. Results demonstrate that there is improvement in classification scores when using time segmentation as a pre-processing step.},
  title={Analysis of drum machine kick and snare sounds},
  author={Shier, Jordie and McNally, Kirk and Tzanetakis, George},
  booktitle={Audio Engineering Society Convention 143},
  year={2017},
  organization={Audio Engineering Society},
  pdf={https://www.aes.org/e-lib/browse.cfm?elib=19284},
  poster={shier2017analysis_poster.pdf}
}

@inproceedings{shier2017sieve,
  abbr={WIMP},
  abstract={The use of electronic drum samples is widespread in contemporary music productions, with music producers having an unprecedented number of samples available to them. To be efficient, users of these large collections require new tools to assist them in sorting, selection and auditioning tasks. This paper presents a new plugin for working with a large collection of kick and snare samples within a music production context. A database of 4230 kick and snare samples, representing 250 individual electronic drum machines are analyzed by segmenting the audio samples into different sample lengths and characterizing these segments using audio feature analysis. The resulting multidimensional feature space is reduced using principle component analysis (PCA). Samples are mapped to a 2D grid interface within an audio plug-in built using the JUCE software framework.},
  title={Sieve: A plugin for the automatic classification and intelligent browsing of kick and snare samples},
  author={Shier, Jordie and McNally, Kirk and Tzanetakis, George},
  booktitle={3rd Workshop on Intelligent Music Production. WIMP},
  year={2017},
  pdf={shier2017sieve_paper.pdf},
  poster={shier2017sieve_poster.pdf},
  code={https://github.com/jorshi/sieve}
}