<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>torchdrum | Jordie Shier</title> <meta name="author" content="Jordie Shier"/> <meta name="description" content="ADC 2024 Poster Presentation. TorchDrum is an audio-driven 808 drum synthesiser built with PyTorch and JUCE."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/icons8-electronic-music-96.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jordieshier.com/projects/adc2024/"> <link href="https://vjs.zencdn.net/8.16.1/video-js.css" rel="stylesheet"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://jordieshier.com/"><span class="font-weight-bold">Jordie</span> Shier</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">torchdrum</h1> <p class="post-description">ADC 2024 Poster Presentation. TorchDrum is an audio-driven 808 drum synthesiser built with PyTorch and JUCE.</p> </header> <article> <div style="padding-bottom:10px"> Jordie Shier<sup>1</sup>, Charalampos Saitis<sup>1</sup>, Andrew Robertson<sup>2</sup>, and Andrew McPherson<sup>3</sup> <div style="font-size:small"> <sup>1</sup>Centre for Digital Music, Queen Mary University of London<br> <sup>2</sup>Ableton AG<br> <sup>3</sup>Dyson School of Design Engineering, Imperial College London<br> </div> </div> <p><br></p> <iframe width="770" height="440" src="https://www.youtube.com/embed/Dj0MtBTQuOU?si=eF4tbVNd6v9-W3SB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <div style="font-size:smaller"> TorchDrum user interface designed by Lewis Wolstanholme and Francis Devine of <a href="https://juliaset.bandcamp.com/" target="_blank" rel="noopener noreferrer">Julia Set.</a> </div> <hr> <div align="left"> <a href="https://github.com/jorshi/torchdrum-plugin" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i> <b>Audio Plugin Code</b></a> | <a href="https://github.com/jorshi/ddsp-timbre-remap" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i> <b>Training Code</b></a> | <a href="https://github.com/jorshi/torchdrum-plugin/releases" target="_blank" rel="noopener noreferrer"><i class="fas fa-headphones"></i> <b>Audio Plugin</b></a> | <a href="https://colab.research.google.com/drive/1nwV5y2eYiCF9YIM1BSmKU9uiPBbw9OxV?usp=sharing" target="_blank" rel="noopener noreferrer"><i class="fas fa-laptop-code"></i> <b>Google Colab</b></a> | <a href="https://arxiv.org/abs/2407.04547" target="_blank" rel="noopener noreferrer"><i class="fas fa-paperclip"></i> <b>Paper</b></a> | <a href="https://youtu.be/IVaNfp4rev0" target="_blank" rel="noopener noreferrer"><i class="fas fa-video"></i> <b>Presentation</b></a> </div> <hr> <h2 id="contents">Contents</h2> <ul> <li><a href="#what-is-torchdrum">What is TorchDrum?</a></li> <li><a href="#demo">Demo</a></li> <li><a href="#timbre-remapping">Timbre Remapping</a></li> <li><a href="#parameter-mapping-neural-network">Parameter Mapping Neural Network</a></li> <li><a href="#creating-a-plugin">Creating a Plugin</a></li> <li><a href="#further-reading">Further Reading</a></li> </ul> <h2 id="what-is-torchdrum">What is TorchDrum?</h2> <p>TorchDrum is an audio plugin that transforms input audio signals into a synthesised 808 drum in real-time. It is a synthesiser that receives audio (not MIDI). The rhythm, loudness, and timbral variations from a percussive input signal are mapped to synthesiser controls, allowing for dynamic, audio-based control. The figure below shows the main signal flow implemented in the audio plug-in, which was built in C++ using JUCE.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/adc2024/real-time-slide-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/adc2024/real-time-slide-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/adc2024/real-time-slide-1400.webp"></source> <img src="/assets/img/adc2024/real-time-slide.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Real-time Plugin Oveview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> The main signal processing and mapping components are:</p> <ul> <li>1) <strong>Onset Detection</strong>: drum hits in the input signal are detected using an onset detection algorithm. This allows the rhythm from the input to be mapped to the synthesiser.</li> <li>2) <strong>Feature Extraction</strong>: a short segment of audio is extracted from the input signal for feature extraction. We only use 256 samples which is ~5.3ms (48kHz sampling rate), this minimises the delay between a detected onset and triggering the synth. This segment of 256 samples is passed into an audio feature extractor that measures the loudness, spectral centroid (brightness), and spectral flatness (noisiness) of the input.</li> <li>3) <strong>Parameter Mapping Neural Network</strong>: These audio features are then passed into a neural network that has been trained to produce synthesiser parameter modulations for the 808 synthesiser so the output follows the variations in loudness and timbre observed at the input.</li> </ul> <p>We refer to the process of automatically updating synthesiser parameters to reflect the timbral variations in the input signal as <em>timbre remapping</em>. <br></p> <hr> <h2 id="demo">Demo</h2> <iframe width="770" height="440" src="https://www.youtube.com/embed/V3ht2JhvJjA?si=ey2OYjWirZM5AGn5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <div style="font-size:smaller"> Drummer: Carson Gant (<a href="https://www.instagram.com/oneupdrumvids/?hl=en" target="_blank" rel="noopener noreferrer">@oneupdrumvids)</a> </div> <p><br></p> <hr> <h2 id="timbre-remapping">Timbre Remapping</h2> <p>Timbre remapping involves the analysis and transfer of timbre from an audio input onto controls for a synthesizer. To achieve timbre remapping we consider musical phrases where timbre changes from note to note. We look at the <em>differences in timbre</em> (and loudness) between notes and try to recreate those differences on our synthesiser. The process is similar to transposing a melody into a different key, but instead of pitch, we are dealing with melodies of timbre and loudness, and are transposing from an input instrument onto synthesiser.</p> <p>Let’s make this a bit more concrete. Here is an example of a short snare drum phrase with plots of the timbre and loudess for each hit. The arrow shows differences between each note compared to the first note in the phrase.</p> <iframe width="770" height="440" src="https://www.youtube.com/embed/X8ivQVtUQQc?si=bO6a6xFqT-2sFDlX&amp;loop=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p>Here’s the same snare drum phase (without the rhythm) followed by all the differences in timbre and loudness mapped onto a synthesiser. What we end up is a new a synthesiser sound (a modulated preset) for each hit in the input phrase.</p> <iframe width="770" height="440" src="https://www.youtube.com/embed/cdY3EB3O7TM?si=GGN6hkTJd0HjTJhh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <p><br></p> <hr> <h2 id="parameter-mapping-neural-network">Parameter Mapping Neural Network</h2> <p>The neural network enables real-time timbre remapping, allowing for input signals to be transformed with low-latency. The neural network we use is relatively small, but it contains non-linearities, allowing for complex relationships between input features and parameters to be formed.</p> <p>The neural network is trained to generate synthesiser parameters to match the loudness and timbral changes over full the full length of a drum hit (~1 second). During real-time operation we can’t wait a full second to see how the input sound fully evolves, so we predict parameters based on short segments of audio at detected onsets.</p> <p>We also employ a recent method called <em>differentiable digital signal processing</em> (DDSP), which enables DSP algorithms to be integrated directly into gradient-descent based optimization used to train neural networks. Practically speaking, this allows us to compute training error on the audio output of our synthesiser using loudness and timbral features. We don’t need to know the parameters for our timbre remapping ahead of time – we only need our input audio which we use during <em>self-supervised</em> training.</p> <p>This figure outlines the training process:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/adc2024/neural-net-training-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/adc2024/neural-net-training-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/adc2024/neural-net-training-1400.webp"></source> <img src="/assets/img/adc2024/neural-net-training.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Neural Network Training" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br> The main takeaway from the figure is that we’re training the neural network to match <em>differences</em> in audio features between two difference hits. \(y = f(x_a) - f(x_b)\) is the difference between two different sounds in an input, and \(\hat{y} = f(x_c) - f(x_d)\) is the difference between a synthesiser preset and a modulated version of that preset. The goal of the neural network is create a parameter modulation \(\theta_{mod}\), which is added to a static preset \(\theta_{pre}\) so that \(y = \hat{y}\). Furthermore, the neural network is trained to make this prediction from onset features \(f_o(\cdot)\) to allow for real-time parameter prediction.</p> <p><br></p> <hr> <h2 id="creating-a-plugin">Creating a Plugin</h2> <p>Neural network training is conducted in Python using <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch</a>. After training, the neural network is exported to <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer">torchscript</a>, allowing it to be loaded into a <a href="https://juce.com/" target="_blank" rel="noopener noreferrer">JUCE Plugin</a> written in C++ with <a href="https://pytorch.org/cppdocs/" target="_blank" rel="noopener noreferrer">torchlib</a>.</p> <p>The components required for real-time inference, namely the onset detection, onset features, and the synthesiser were rewritten in C++ for the audio plug-in. To make sure the Python and C++ matched, we wrote unit tests that loaded the C++ into Python using <a href="https://cppyy.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">cppyy</a> and compare the outputs.</p> <p>Here’s a visual overview of the components required for training in Python and what was included in the real-time plug-in.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/adc2024/implementation-details.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/adc2024/implementation-details.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/adc2024/implementation-details.gif-1400.webp"></source> <img src="/assets/img/adc2024/implementation-details.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Implementation Details" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br></p> <hr> <h2 id="further-reading">Further Reading</h2> <p>If you’re interested in learning more deeply about this research and it’s background, we first recommend checking out the <a href="https://www.youtube.com/watch?v=IVaNfp4rev0&amp;ab_channel=JordieShier" target="_blank" rel="noopener noreferrer">conference video presentation</a> and the <a href="https://arxiv.org/abs/2407.04547" target="_blank" rel="noopener noreferrer">paper</a>, which was published at the 2024 New Interfaces for Musical Expression conference. You can find a more detailed description of the methods and all the background references.</p> <p>If you’re more of a code person then check out this <a href="https://colab.research.google.com/drive/1nwV5y2eYiCF9YIM1BSmKU9uiPBbw9OxV?usp=sharing" target="_blank" rel="noopener noreferrer">Google Colab</a> which provides a tutorial on training new mapping models. Repositories containing <a href="https://github.com/jorshi/ddsp-timbre-remap" target="_blank" rel="noopener noreferrer">training code</a> and the <a href="https://github.com/jorshi/torchdrum-plugin" target="_blank" rel="noopener noreferrer">audio plugin</a> are also available.</p> <p>Below is a brief curated list of papers and resources that were most influential in the design of this research.</p> <p><strong>Differentiable Digital Signal Processing</strong></p> <ul> <li> <p>Engel, Jesse, et al. “<a href="https://arxiv.org/abs/2001.04643" target="_blank" rel="noopener noreferrer">DDSP: Differentiable digital signal processing.</a>” arXiv preprint arXiv:2001.04643 (2020).</p> </li> <li> <p>Hayes, Ben, et al. “<a href="https://doi.org/10.3389/frsip.2023.1284100" target="_blank" rel="noopener noreferrer">A review of differentiable digital signal processing for music and speech synthesis.</a>” Frontiers in Signal Processing 3 (2024): 1284100.</p> </li> <li> <p>Hayes, Ben, et al. “<a href="https://intro2ddsp.github.io/intro.html" target="_blank" rel="noopener noreferrer">Introduction to DDSP for Audio Synthesis.</a>” ISMIR Tutorial (2023).</p> </li> </ul> <p><strong>Timbre Space, Timbre Analogies, and Timbre Remapping</strong></p> <ul> <li> <p>Grey, John M. “<a href="https://doi.org/10.1121/1.381428" target="_blank" rel="noopener noreferrer">Multidimensional perceptual scaling of musical timbres.</a>” the Journal of the Acoustical Society of America 61.5 (1977): 1270-1277.</p> </li> <li> <p>Wessel, David L. “<a href="https://www.jstor.org/stable/3680283" target="_blank" rel="noopener noreferrer">Timbre space as a musical control structure.</a>” Computer music journal (1979): 45-52.</p> </li> <li> <p>Stowell, Dan, and Mark D. Plumbley. “<a href="https://scholar.google.com/scholar_url?url=https://www.academia.edu/download/41516099/7.pdf&amp;hl=en&amp;sa=T&amp;oi=gsb-ggp&amp;ct=res&amp;cd=1&amp;d=11065372655903924951&amp;ei=eqgkZ_npOdmDy9YP0Zu88QU&amp;scisig=AFWwaebLvcShov4eQL73pI1Shi2a" target="_blank" rel="noopener noreferrer">Timbre remapping through a regression-tree technique.</a>” Sound and Music Computing (SMC) (2010).</p> </li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Jordie Shier. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. <a target="_blank" href="https://icons8.com/icon/12100/electronic-music" rel="noopener noreferrer">Electronic Music</a> icon by <a target="_blank" href="https://icons8.com" rel="noopener noreferrer">Icons8</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>